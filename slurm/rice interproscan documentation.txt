IPS
0. Run: Did this weekend's experiment work?
1. Time: looks like each job finishes in an hour. Do we know how variable that is? Will affect batching the jobs for the array. I can write a little python batch splitter for an array of N jobs if that would be helpful?
2. Resources: Does it really need 8 CPU's to run a job? Or could we scale that down without much of a performance loss, in order to give the horizontal array scaling more room?
3. Build: Is it worth trying to run the older version of this thing that they are using, in order to avoid the oddities we're running into with the output files?



file shipping to rdf from nots
-- they're going to see how to do that.



There is a known issue with xalt: it causes perl to crash unless you turn off its logging. A misleading wrinkle to this is that even if you export all your env variables to the job on interactive you still have to do it manually, whereas with slurm, you can turn off the xalt logging and this env variable carries over.


runs in stages:
0. install & environment
0a. install
0a1. download repo into projects, set up environment (see 0b below)
0a2. run initial_setup.py in an interactive job
0b. environment
0b1. Java > 11
0b2. CGGcore/8.3.0
0b3. Python/3.7.2
0b4. Perl/5.30.0
0b5. cluster mode does not appear to work with slurm
1. run initial_setup.py 
1. generating matches
1a. java process spawns threads, probably talking to their remote database which is faster than...
1b. running this locally
2. creates tons of small temp files, databases, etc.
2a. which runs fastest if it's writing to /tmp instead of /scratch (3 vs. 1-1.5 hours)
2b. and if it's running on a single node, where it appears to multithread well
3. input and output files, relatively small, can be read/written to/from /scratch

Validation


Pre and Post-processing
--how do they know which of the zip files to stage in and out for each of these work batches, and can that be programmatically staged
--do we have a master list of these *.faa and corresponding *.faa.tsv for us to do:
----that programmatic staging
----pre-processing job distribution
----post-processing job completion check, cleanup, export, and bit-flipping

Potential down-the-road optimization:
1. we could maybe set up a local instance of the database that has to go out to England as a query, to reduce the overhead that's reliant on network latency.
2. either way, could we avoid re-running the first database query step every single time to the external database to cut down on the overhead?

Next steps for Annelise and Emily:
1. Validation
1a. do our results match yours (different filesize, different software versions)
1b. for down the road think about: what is your validation process and can it be automated
2. Familiarity with submitting jobs
2a. examine slurm script
2b. examine file structure
3. Trying different node settings (using the same input file) for:
3a. Resource basic requirements (how few cores & how little RAM can you run this with)
3b. Performance improvement on resources (how much does this speed up your job)
4. How much variability do we see on 3--use a random selection of a few input files to:
4a. See if your basic requirements change -- does it crash? -- very unlikely
4b. See how much your run-time varies -- does it change? -- likely
**consider running 3 and 4 on scavenge queue in order to get results faster
**and consider ssh'ing into the nodes for step 3 especially to see resource usage

for array
--probably want to set --tasks-per-core=1 and nodes=ntasks
--make INP variable into a parameter rather than a hard-coded filename
--change output filename to correspond to that INP variable
